# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bE8A0vBeW8kOyejwb_rvtEiLe4wBimEX
"""

# This assignment is done by Jad Alaouie & Bilal Osman

"""The Adult Income dataset, available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult), is a widely recognized and extensively studied dataset in the field of Machine Learning and Data Analysis. This dataset offers valuable insights into the socio-economic and demographic factors influencing an individuals's income, making it a valuable resource for researchers, data scientists, and analysts. Comprising a rich set of attributes, including age, education, occupation, marital status, and more, the dataset enables comprehensive exploration of patterns, predictions, and classifications related to income levels in the adult population. This introduction provides an overview of the dataset's source, structure, and its significance as a foundation for understanding the complex interplay of variables affecting income outcomes.

The Adult Income dataset has been used in numerous studies and projects to predict income levels, perform demographic analysis, and gain insights into economic disparities. Its application extends across fields such as social science, economics, and data science, and it serves as an essential resource for building and evaluating predictive models.

In this task, we'll enhance the code developed in Assignment 2 for better organization. Subsequently, we'll commence by implementing various classification models, including a rule-based classifier, decision tree, random forest, and a basic feed-forward neural network.

![UCI Machine Learning.png](data:image/png;base64,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)
"""

!pip install ucimlrepo

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ucimlrepo import fetch_ucirepo

adult = fetch_ucirepo(id = 2)
features = adult.data.features
targets = adult.data.targets

DataFrame = pd.DataFrame(features)
DataFrame.insert(14,'income',targets)
DataFrame.head(10)

# In assignment 2 we used two methods to address missing values (Statistical Approach or Machine Learning Approach)

# Since the number of missing values is small we will use basic Statistical Approaches :

Workclass_Mode = DataFrame['workclass'].mode()
DataFrame['workclass'].replace('?', 'Private', inplace = True)
DataFrame['workclass'].fillna('Private', inplace = True)
Occupation_Mode = DataFrame['occupation'].mode()
DataFrame['occupation'].replace('?', 'Prof-specialty', inplace = True)
DataFrame['occupation'].fillna('Prof-specialty', inplace = True)

# For Native-Country feature we will just drop the rows because they represent a very small value :

DataFrame.dropna(subset = ['native-country'], inplace = True)

# Checking if values were replaced and we no longer have any missing data :

missing_values = DataFrame.isna().sum()
print("Missing Values in each feature:")
print(missing_values)

DataFrame.head()

DataFrame['sex'] = DataFrame.sex.replace({"Female" : 0, "Male" : 1})
DataFrame['income'] = DataFrame.income.replace({"<=50K." : 0,"<=50K" : 0, ">50K" : 1,">50K." : 0})

def map_native_country(country):
    return 1 if country == 'United-States' else 0

DataFrame['native-country'] = DataFrame['native-country'].apply(map_native_country)

DataFrame['marital-status'] = DataFrame['marital-status'].replace(['Divorced','Married-spouse-absent','Never-married','Separated','Widowed'],'Single')
DataFrame['marital-status'] = DataFrame['marital-status'].replace(['Married-AF-spouse','Married-civ-spouse'],'Couple')

relationship_map = {'Unmarried':0,'Wife':1,'Husband':2,'Not-in-family':3,'Own-child':4,'Other-relative':5}
DataFrame['relationship'] = DataFrame['relationship'].map(relationship_map)

marital_status_map = {'Single' : 1, 'Couple' : 0}
DataFrame['marital-status'] = DataFrame['marital-status'].map(marital_status_map)

race_mapping = {
    'White': 0,
    'Asian-Pac-Islander': 1,
    'Amer-Indian-Eskimo': 2,
    'Black': 3,
    'Other': 4
}

DataFrame['race'] = DataFrame['race'].map(race_mapping)

DataFrame.head()

workclass_mapping = {
    'Private': 'private',
    'Self-emp-not-inc': 'self_employed',
    'Self-emp-inc': 'self_employed',
    'Federal-gov': 'govt',
    'Local-gov': 'govt',
    'State-gov': 'govt',
    'Without-pay': 'without_pay',
    'Never-worked': 'without_pay'
}

DataFrame['workclass'] = DataFrame['workclass'].replace(workclass_mapping)

workclass_map = {'govt' : 0, 'private' : 1, 'self_employed' : 2, 'without_pay' : 3}
DataFrame['workclass'] = DataFrame['workclass'].map(workclass_map)

DataFrame.drop(labels = ['education', 'occupation', 'fnlwgt'], axis = 1, inplace = True)

DataFrame.head()

# Feature Scaling :
from sklearn.preprocessing import StandardScaler, LabelEncoder

categorical_columns = ['workclass', 'marital-status', 'relationship', 'race', 'sex', 'native-country']
label_encoder = LabelEncoder()
for column in categorical_columns:
    DataFrame[column] = label_encoder.fit_transform(DataFrame[column])

X = DataFrame.drop('income', axis=1)
y = DataFrame['income']

numerical_columns = X.columns.difference(categorical_columns)
scaler = StandardScaler()
X[numerical_columns] = scaler.fit_transform(X[numerical_columns])

X.head()

DataFrame.head()

# Splitting Data into Train & Test :
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 40)

from sklearn.dummy import DummyClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier

# Baseline Rule-Based Classifier
baseline_classifier = DummyClassifier(strategy = 'most_frequent')

baseline_scores = cross_val_score(baseline_classifier, X, y, cv= RepeatedStratifiedKFold(n_splits = 5, n_repeats = 2, random_state = 42))

# Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state = 42)

decision_tree_param_grid = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
decision_tree_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)
decision_tree_grid_search = GridSearchCV(decision_tree, decision_tree_param_grid, cv=decision_tree_cv)
decision_tree_grid_search.fit(X_train, y_train)
dt_scores = cross_val_score(decision_tree_grid_search.best_estimator_, X, y, cv=decision_tree_cv)

# Feed Forward Neural Network
# expected time = 3 hours
from sklearn.neural_network import MLPClassifier

mlp_classifier = MLPClassifier(random_state=42, max_iter=2000, learning_rate='adaptive', alpha=0.0001)

mlp_param_grid = {'hidden_layer_sizes': [(50,), (100,), (50, 50)],
                  'activation': ['logistic', 'tanh', 'relu'], 'alpha': [0.0001, 0.001, 0.01]}
mlp_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)
mlp_grid_search = GridSearchCV(mlp_classifier, mlp_param_grid, cv=mlp_cv)
mlp_grid_search.fit(X_train, y_train)
mlp_scores = cross_val_score(mlp_grid_search.best_estimator_, X, y, cv=mlp_cv)

# Random Forest Classifier
# expected time = 40 minutes to 1 hour
random_forest = RandomForestClassifier(random_state = 42)

random_forest_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30],
                  'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
random_forest_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)
random_forest_grid_search = GridSearchCV(random_forest, random_forest_param_grid, cv=random_forest_cv)
random_forest_grid_search.fit(X_train, y_train)
rf_scores = cross_val_score(random_forest_grid_search.best_estimator_, X, y, cv=random_forest_cv)

"""1.a- **Hyperparameters**:

a-**Decision Tree Hyperparameters:**

**max_depth:** [None, 10, 20, 30]

**Justification:** The max_depth parameter controls the maximum depth of the decision tree. Setting it to None allows nodes to expand until they contain fewer than min_samples_split samples. By exploring depths of 10, 20, and 30, the model can capture different levels of complexity. The intention is to prevent overfitting by limiting the depth.

**min_samples_split:** [2, 5, 10]

**Justification:** This parameter sets the minimum number of samples required to split an internal node. A lower value can lead to more complex trees, potentially overfitting the training data. By trying values of 2, 5, and 10, the model explores the impact on the balance between complexity and overfitting.

**min_samples_leaf:** [1, 2, 4]

**Justification:** This parameter sets the minimum number of samples required to be at a leaf node. It helps control the size of the leaves and can impact the balance between overfitting and generalization. Trying values of 1, 2, and 4 allows for a range of leaf sizes to be explored.


b-**Random Forest Classifier**:

**n_estimators:** [50, 100, 200]

**Justification:** This parameter determines the number of trees in the forest. A higher number of trees generally leads to better performance, but it also increases computational cost. By exploring values of 50, 100, and 200, the code considers a range of ensemble sizes.

**max_depth:** [None, 10, 20, 30]

**Justification:** Similar to the decision tree, controlling the maximum depth of individual trees in the forest helps prevent overfitting. Exploring depths of 10, 20, and 30 allows the model to capture different levels of complexity.

**min_samples_split:** [2, 5, 10]

**Justification:** Similar to the decision tree, this parameter sets the minimum number of samples required to split an internal node in individual trees. Exploring different values provides flexibility in controlling tree complexity.

**min_samples_leaf:** [1, 2, 4]

**Justification:** Similar to the decision tree, this parameter sets the minimum number of samples required to be at a leaf node in individual trees. It helps control leaf size and generalization.

c-**MLP(Feed Forward Neural Network) Hyperparameters**:

**hidden_layer_sizes:** [(50,), (100,), (50, 50)]

**Justification:** This parameter defines the number of neurons in the hidden layers of the neural network. By trying different combinations like (50,), (100,), and (50, 50), the model explores various network architectures.

**activation:** ['logistic', 'tanh', 'relu']

**Justification:** This parameter determines the activation function for the hidden layers. Different activation functions can impact the model's ability to learn complex patterns. Trying 'logistic', 'tanh', and 'relu' covers common choices.

**alpha:** [0.0001, 0.001, 0.01]

**Justification:** The alpha parameter is the regularization term that helps prevent overfitting. Trying values of 0.0001, 0.001, and 0.01 explores different levels of regularization.

1.b- **Decision Tree Classifier:**

**Stratified Cross-Validation:**

**RepeatedStratifiedKFold** is used, ensuring that each fold maintains the same class distribution as the entire dataset, which is crucial for classification problems.
**Grid Search**:

**GridSearchCV **is employed to search through the provided hyperparameter grid for the Decision Tree model.
**Standard Deviation Consideration:**

**The standard deviation** of the cross-validated scores (dt_scores) is implicitly considered. The use of RepeatedStratifiedKFold helps assess the variability in model performance.

1.b- **Feed Forward Neural Network(MLP):**

**Stratified Cross-Validation**:

Similar to the Decision Tree, the Feed Forward Neural Network uses **RepeatedStratifiedKFold** to ensure fair representation of classes in each fold.

**Grid Search**:

**GridSearchCV** is used to explore the provided hyperparameter grid for the MLP model.

**Standard Deviation Consideration:**

**The standard deviation** of the cross-validated scores (mlp_scores) is implicitly considered, providing insights into the variability of the model's performance.

1.b- **Random Forest Classifier:**

**Stratified Cross-Validation:**

Similar to the previous models, RepeatedStratifiedKFold ensures proper class representation in each fold.

**Grid Search**:

GridSearchCV explores the provided hyperparameter grid for the Random Forest model.

**Standard Deviation Consideration:**

The standard deviation of the cross-validated scores (rf_scores) is implicitly considered, providing insights into the variability of the model's performance.

1.c- **[Decision Tree - Feed Forward Neural Network(MLP) - Random Forest Classifier]**

**Performance Metric** The hyperparameter search for the 3 models is configured for accuracy.

**Accuracy** is a common and intuitive metric for classification problems. It measures the overall correctness of predictions by considering both true positives and true negatives.

**(Decision Tree) Balanced Classes:** The use of RepeatedStratifiedKFold suggests a consideration for balanced class distributions in each fold, making accuracy an appropriate metric.

**(Feed Forward Neural Network(MLP)) Balanced Classes:**  The use of RepeatedStratifiedKFold suggests that the classes are considered to be balanced during cross-validation.

**Random Forest Classifier Use in Ensembles:** Random Forest, being an ensemble method, benefits from accuracy as it aggregates predictions from multiple trees.

Therefore, the performance metric chosen for the hyperparameter search in all three models (Decision Tree, MLP, Random Forest) is accuracy. This choice aligns with the use of RepeatedStratifiedKFold, indicating consideration for balanced classes and the desire for an overall measure of correct predictions across different folds and repetitions.

1-d **Avoiding Data Leakage**

Data leakage occurs when information from the test set is inadvertently used during the model training phase, leading to overly optimistic performance estimates. To ensure no data leakage:

1. We split the dataset into training and testing sets using 'train_test_split'. The 'test_size' parameter is set to 30%, meaning 70% of the data is used for training and 30% for testing.

2. The hyperparameter search for the Decision Tree model ('GridSearchCV') is performed using cross-validation on the training set('X_train', 'y_train')

3. RepeatedStratifiedKFold is used, ensuring that the corss-validition maintains the same class distribution as the entire training set.

4. Independent Evaluation of Other Models: Similar practices are applied for the Feed Forward Neural Network and Random Forest models.
"""

from sklearn.metrics import accuracy_score

baseline_classifier.fit(X_train, y_train)
baseline_pred = baseline_classifier.predict(X_test)
baseline_accuracy = accuracy_score(y_test, baseline_pred)

# Decision Tree
dt_pred = decision_tree_grid_search.best_estimator_.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)

# Random Forest
rf_pred = random_forest_grid_search.best_estimator_.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

# Neural Network
mlp_classifier.fit(X_train, y_train)
mlp_pred = mlp_classifier.predict(X_test)
mlp_accuracy = accuracy_score(y_test, mlp_pred)
print("Baseline Classifier Accuracy: {:.3f}".format(baseline_accuracy))
print("Decision Tree Accuracy: {:.3f}".format(dt_accuracy))
print("Random Forest Accuracy: {:.3f}".format(rf_accuracy))
print("Neural Network Accuracy: {:.3f}".format(mlp_accuracy))

from sklearn.metrics import precision_score, recall_score

# Baseline Classifier
baseline_precision = precision_score(y_test, baseline_pred)
baseline_recall = recall_score(y_test, baseline_pred)

# Decision Tree
dt_precision = precision_score(y_test, dt_pred)
dt_recall = recall_score(y_test, dt_pred)

# Random Forest
rf_precision = precision_score(y_test, rf_pred)
rf_recall = recall_score(y_test, rf_pred)

# Neural Network
nn_precision = precision_score(y_test, mlp_pred)
nn_recall = recall_score(y_test, mlp_pred)
print("Baseline Classifier Precision: {:.3f}, Recall: {:.3f}".format(baseline_precision, baseline_recall))
print("Decision Tree Precision: {:.3f}, Recall: {:.3f}".format(dt_precision, dt_recall))
print("Random Forest Precision: {:.3f}, Recall: {:.3f}".format(rf_precision, rf_recall))
print("Neural Network Precision {:.3f}, Recall: {:.3f}".format(nn_precision, nn_recall))

# Sensitivity is Recall for positive class
sensitivity_dt = dt_recall
sensitivity_rf = rf_recall
sensitivity_nn = nn_recall
# Specificity for binary classification is Recall for negative class
specificity_dt = recall_score(1 - y_test, 1 - dt_pred)
specificity_rf = recall_score(1 - y_test, 1 - rf_pred)

# Specificity for Neural Network
tn_mlp = np.sum((1 - y_test) & (1 - mlp_pred))
fp_mlp = np.sum((1 - y_test) & mlp_pred)

specificity_mlp = tn_mlp / (tn_mlp + fp_mlp)



print("Decision Tree Sensitivity: {:.3f}, Specificity: {:.3f}".format(sensitivity_dt, specificity_dt))
print("Random Forest Sensitivity: {:.3f}, Specificity: {:.3f}".format(sensitivity_rf, specificity_rf))
print("Neural Network Specificity: {:.3f}".format(sensitivity_nn,specificity_mlp))

from sklearn.metrics import f1_score, roc_auc_score

# Baseline Classifier
baseline_f1 = f1_score(y_test, baseline_pred)
baseline_auc_roc = roc_auc_score(y_test, baseline_classifier.predict_proba(X_test)[:, 1])

# Decision Tree
dt_f1 = f1_score(y_test, dt_pred)
dt_auc_roc = roc_auc_score(y_test, decision_tree_grid_search.best_estimator_.predict_proba(X_test)[:, 1])

# Random Forest
rf_f1 = f1_score(y_test, rf_pred)
rf_auc_roc = roc_auc_score(y_test, random_forest_grid_search.best_estimator_.predict_proba(X_test)[:, 1])

# Neural Network
mlp_f1 = f1_score(y_test, mlp_pred)
mlp_auc_roc = roc_auc_score(y_test, mlp_classifier.predict_proba(X_test)[:, 1])

print("Baseline Classifier F1 Score: {:.3f}, AUC ROC: {:.3f}".format(baseline_f1, baseline_auc_roc))
print("Decision Tree F1 Score: {:.3f}, AUC ROC: {:.3f}".format(dt_f1, dt_auc_roc))
print("Random Forest F1 Score: {:.3f}, AUC ROC: {:.3f}".format(rf_f1, rf_auc_roc))
print("Neural Network F1 Score: {:.3f}, AUC ROC: {:.3f}".format(mlp_f1, mlp_auc_roc))

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curve(estimator, title, X, y, cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt

# Decision Tree Learning Curve
title = "Decision Tree Learning Curve"
plot_learning_curve(decision_tree_grid_search.best_estimator_, title, X, y, cv=decision_tree_cv, n_jobs=-1)
plt.show()

# Random Forest Learning Curve
title = "Random Forest Learning Curve"
plot_learning_curve(random_forest_grid_search.best_estimator_, title, X, y, cv=random_forest_cv, n_jobs=-1)
plt.show()

# Neural Network Learning Curve
title = "Neural Network Learning Curve"
plot_learning_curve(mlp_grid_search.best_estimator_, title, X, y, cv = mlp_cv, n_jobs = -1)
plt.show()

"""2.a- Overall Accuracy:

    Decision Tree Accuracy: 0.857

    Neural Network (MLP) Accuracy: 0.853

    Random Forest Accuracy: 0.860

    Baseline Classifier Accuracy: 0.841

All models achieve relatively high overall accuracy, indicating a significant proportion of correct predictions.

2.b-Precision and Recall:

    Decision Tree:
                  Precision: 0.593
                  Recall: 0.331

    Neural Network (MLP):
                  Precision: 0.587
                  Recall: 0.258
        
    Random Forest:
                  Precision: 0.604
                  Recall: 0.355
        
    Baseline Classifier
                  Precision: 0.000
                  Recall: 0.000

We notice that the Baseline Classifier has the lowest Precision and Recall values, and the reason is by always predicting the most frequent class, the classifier will never correctly identify instances of the minority class (>50K). As a result, both precision and recall are 0. This is a common characteristic of baseline classifiers, especially when they predict the majority class for imbalanced datasets. In addition to that, Decision Tree and Random Forest show higher precision and recall compared to the Neural Network. However, further experimentation and tuning of the neural network architecture and hyperparameters could potentially improve its performance.

2.c- Sensitivity and Specificity:



    Decision Tree:
            Sensitivity(Recall for Positive Class): 0.331
            Sensitivity(Recall for Negative Class): 0.957
    
    Neural Network (MLP):
            Sensitivity(Recall for Positive Class): 0.258
            Sensitivity(Recall for Negative Class): 0.965
    
    Random Forest:
            Sensitivity(Recall for Positive Class): 0.355
            Sensitivity(Recall for Negative Class): 0.956


Sensitivity, also known as recall or true positive rate, measures the ability of a model to correctly identify positive instances out of all actual positive instances.

Decision Tree and Random Forest have higher sensitivity values compared to the Neural Network. This implies that these tree-based models are better at capturing individuals with an income level greater than 50K (positive class) compared to the Neural Network.

A higher sensitivity indicates a better ability to avoid false negatives, i.e., the models are better at not missing individuals with higher incomes.


Specificity, also known as the true negative rate, measures the ability of a model to correctly identify negative instances out of all actual negative instances.

All models, including the Neural Network, have high specificity values, indicating that they are good at correctly identifying individuals with an income level less than or equal to 50K (negative class).

Neural Network has the highest specificity among the three models, suggesting that it is slightly better at avoiding false positives, i.e., correctly identifying individuals with lower incomes.

2.d- F1 Score and AUC ROC:

    Decision Tree:
      F1 Score: 0.425
      AUC ROC: 0.849

    Neural Network (MLP):
      F1 Score: 0.359
      AUC ROC: 0.861

    Random Forest:
      F1 Score: 0.447
      AUC ROC: 0.871

    Baseline Classifier
      F1 Score: 0.000
      AUC ROC: 0.500


F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both false positives and false negatives.

Random Forest has the highest F1 score, indicating a good balance between precision and recall. This means it performs well in capturing positive instances while avoiding false positives.

Decision Tree follows with a moderate F1 score, and the Neural Network has a slightly lower F1 score.


AUC ROC (Area Under the Receiver Operating Characteristic curve) measures the model's ability to discriminate between positive and negative instances across different probability thresholds.

Random Forest has the highest AUC ROC, indicating strong discriminative ability. It performs well in distinguishing between individuals with income levels greater than 50K and those with income levels less than or equal to 50K.

The Neural Network follows closely, and the Decision Tree also demonstrates good discriminatory power.


**Cost Consideration:**

The mistakes in predicting the positive class (<=50K) may be more costly than predicting the negative class (>50K) because incorrectly classifying someone as earning less when they actually earn more might have financial implications.

In the context of this problem, a false positive (predicting someone earns <=50K when they actually earn >50K) might lead to missed opportunities or benefits for individuals. This could be more costly than a false negative (predicting someone earns >50K when they actually earn <=50K), which might result in offering unnecessary benefits.

The choice of model can be influenced by the specific business context and the consequences of these false positives and false negatives in the given application.

2.e-i **Learning Curve Decision Tree:**

1. **Bias and Variance:**

    5000 Training Examples(approx):
      Cross Validation Score: 0.83(approx)
      Training Score: 0.87(approx)

At this point, the model is performing reasonably well on both the training and cross-validation sets. The scores are close, indicating a balanced model with moderate complexity.

    12500 Training Examples(approx):
      Cross Validation Score: 0.845(approx)
      Training Score: 0.864(approx)

Both scores have increased, suggesting improved model performance. The gap between the two scores is not large, indicating a good balance, and there is no clear sign of overfitting.

    22000 Training Examples(approx):
      Cross Validation Score: 0.846(approx)
      Training Score: 0.865(approx)

Similar to the previous step, both scores are consistently high, and the small gap suggests a model that generalizes well to unseen data.

    30000 Training Examples(approx):
      Cross Validation Score: 0.85(approx)
      Training Score: 0.861(approx)

The scores continue to be high, and the slight decrease in the training score indicates a small degree of overfitting. However, the model still performs well on the cross-validation set.

    38000 Training Examples(approx):
      Cross Validation Score: 0.859(approx)
      Training Score: 0.868(approx)

Both scores have increased, indicating that with more data, the model generalizes better. The small gap suggests reasonable complexity without significant overfitting.

2. **Trends in Learning Curves:**
Overall Trend:

As the number of training examples increases, both training and cross-validation scores improve, indicating better model performance with more data.
Bias and Variance Trend:

The initially observed bias is reduced as more data becomes available.
The small gap between training and cross-validation scores suggests low variance, indicating a model that generalizes well to new data.

**ii- Learning Curve Random Forest Learning Curve:**

1. **Bias and Variance:**

    5000 Training Examples:
      Cross Validation Score: 0.843
      Training Score: 0.881

Interpretation: The model starts with a high training score and a slightly lower cross-validation score. This suggests some overfitting, but the performance on the cross-validation set is still reasonable.

    12500 Training Examples:
      Cross Validation Score: 0.847
      Training Score: 0.882
Interpretation: Both scores have increased, and the gap has reduced, indicating improved generalization to new data. The model is becoming more balanced.

    22000 Training Examples:
      Cross Validation Score: 0.846
      Training Score: 0.883

Interpretation: The model's performance is consistent, with a small gap between training and cross-validation scores. The complexity seems appropriate, and overfitting is not prominent.

    30000 Training Examples:
      Cross Validation Score: 0.855
      Training Score: 0.875
Interpretation: Both scores are high, and the gap remains small. The model is demonstrating good generalization, and the slight decrease in training score suggests a well-controlled level of complexity.

    39000 Training Examples:
      Cross Validation Score: 0.861
      Training Score: 0.879

Interpretation: Both scores continue to increase, indicating that the model benefits from additional data. The small gap suggests that the model is not overfitting significantly.

2. **Trends in Learning Curves**:

Overall Trend:

Similar to the Decision Tree, as the number of training examples increases, both training and cross-validation scores improve, indicating better model performance with more data.
Bias and Variance Trend:

The initial overfitting observed in the training set diminishes with more data, and the model generalizes better to unseen examples.
The decreasing gap between training and cross-validation scores suggests improved balance.

iii- **Learning Curve Neural Network (MLP)**:


Analysis of Neural Network (MLP) Learning Curve:
1. **Bias and Variance:**

    5000 Training Examples:
      Cross Validation Score: 0.833
      Training Score: 0.8625

Interpretation: The model starts with a slightly higher training score compared to the cross-validation score, suggesting potential overfitting. The performance is decent, but there is room for improvement.

    12500 Training Examples:
      Cross Validation Score: 0.839
      Training Score: 0.856

Interpretation: Both scores have increased, and the gap has reduced, indicating improved generalization. The model is becoming more balanced.

    22000 Training Examples:
      Cross Validation Score: 0.84
      Training Score: 0.8557

Interpretation: The model's performance is consistent, with a small gap between training and cross-validation scores. The complexity seems appropriate, and overfitting is not prominent.

    30000 Training Examples:
      Cross Validation Score: 0.848
      Training Score: 0.854

Interpretation: Both scores are high, and the gap remains small. The model is demonstrating good generalization, and the slight decrease in training score suggests a well-controlled level of complexity.

    39000 Training Examples:
      Cross Validation Score: 0.8558
      Training Score: 0.86

Interpretation: Both scores continue to increase, indicating that the model benefits from additional data. The small gap suggests that the model is not overfitting significantly.

2. **Trends in Learning Curves**:
Overall Trend:

Similar to the Decision Tree and Random Forest, as the number of training examples increases, both training and cross-validation scores improve, indicating better model performance with more data.
Bias and Variance Trend:

The initial signs of overfitting observed in the training set diminish with more data, and the model generalizes better to unseen examples.
The decreasing gap between training and cross-validation scores suggests improved balance.

**NOTE** Creating a learning curve for a baseline classifier might not be as informative or meaningful as for more complex models. The purpose is to observe how the model's performance changes with varying amounts of training data. A baseline classifier which makes predictions based on a simple rule or strategy does not benefit from additional training data.
"""

# Extract the best Decision Tree model from the grid search
best_decision_tree_model = decision_tree_grid_search.best_estimator_

# Extract feature names
feature_names = X.columns

# Visualize the Decision Tree rules
from sklearn.tree import export_text
tree_rules = export_text(best_decision_tree_model, feature_names=list(feature_names))
print(tree_rules)

"""**Marital Status and Education Level:**

**Rule:** marital-status <= 0.50 and education-num <= 0.94

**Interpretation:** The model considers individuals with lower marital status and lower education levels.
Relevance: This aligns with the common expectation that marital status and education level might influence income.

**Capital Gain and Capital Loss:**

Rule: capital-gain <= 0.54 and capital-loss <= 4.23

**Interpretation:** The model considers low capital gain and low capital loss as factors for predicting the class.
Relevance: Financial indicators such as capital gain and loss are reasonable predictors of income.

**Age and Work Hours:**

Rule: age <= -0.08 and hours-per-week <= 0.09

**Interpretation:** For younger individuals working fewer hours, the predicted class is 0.
Relevance: Younger individuals with fewer working hours might be associated with lower income.

**Race and Age:**

Rule: race <= 0.50 and age <= -0.45

**Interpretation:** For a specific race and age range, the predicted class is 0.
Relevance: The model seems to capture income patterns based on race and age, but it's essential to approach such features cautiously to avoid biased predictions.
Native Country and Workclass:

Rule: native-country <= 0.50 and workclass <= 1.50

**Interpretation:** The model considers native country and workclass for predicting the class.
Relevance: These factors might have an impact on income levels.
Hours per Week and Relationship:

Rule: hours-per-week <= 2.47 and relationship <= 3.00
**Interpretation:** For individuals working fewer hours and having a specific relationship status, the predicted class is 0.
Relevance: Work hours and relationship status could influence income.

These rules generally make sense and align with common expectations about factors affecting income.However, its crucial to note potential biases, especially when interpreting rules related to sensitive features like race.
"""